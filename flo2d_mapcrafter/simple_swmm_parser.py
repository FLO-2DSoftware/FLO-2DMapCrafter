# -*- coding: utf-8 -*-
"""
/***************************************************************************
 FLO2DMapCrafter
                                 A QGIS plugin
 This plugin creates maps from FLO-2D output files.
 Generated by Plugin Builder: http://g-sherman.github.io/Qgis-Plugin-Builder/
                              -------------------
        begin                : 2023-09-21
        git sha              : $Format:%H$
        copyright            : (C) 2023 by FLO-2D
        email                : contact@flo-2d.com
 ***************************************************************************/

/***************************************************************************
 *                                                                         *
 *   This program is free software; you can redistribute it and/or modify  *
 *   it under the terms of the GNU General Public License as published by  *
 *   the Free Software Foundation; either version 2 of the License, or     *
 *   (at your option) any later version.                                   *
 *                                                                         *
 ***************************************************************************/
"""

import os
import re
from datetime import datetime

# Flexible regex patterns for RPT
_TIME_NODELINK = re.compile(
    r'^\s*([A-Z]{3}-\d{2}-\d{4})\s+(\d{2}):(\d{2}):\d{2}\s+(.+)$'
)
_NODE_MARK = re.compile(r'^\s*<<<\s*Node\s+(.+?)\s*>>>', re.I)
_LINK_MARK = re.compile(r'^\s*<<<\s*Link\s+(.+?)\s*>>>', re.I)

class SimpleSWMMModel:
    """
    Minimal SWMM parser for MapCrafter:
      - INP: nodes, links, coordinates, link from/to (for adjacency)
      - RPT: Node Flooding Summary, Link Flow Summary (max metrics)
      - RPT: Node Results & Link Results time-series (header-driven)
    """

    def __init__(self, inp_path, rpt_path=None):
        self.inp_path = inp_path
        self.rpt_path = rpt_path

        # Identifiers
        self.nodes = []
        self.links = []

        # Geometry & topology
        self.node_xy = {}
        self.link_from_to = {}

        # Summaries
        self.node_results = {}
        self.link_results = {}

        # Time series
        self.node_ts = {}
        self.link_ts = {}

        if os.path.isfile(inp_path):
            self._parse_inp()
        if rpt_path and os.path.isfile(rpt_path):
            self._parse_rpt()

    # ------------
    # INP parsing
    # ------------
    def _parse_inp(self):
        section = None
        node_sections = {"JUNCTIONS", "OUTFALLS", "STORAGE", "DIVIDERS"}
        link_sections = {"CONDUITS", "PUMPS", "ORIFICES", "WEIRS"}

        with open(self.inp_path, "r", errors="ignore") as f:
            for raw in f:
                line = raw.strip()
                if not line or line.startswith(";"):
                    continue

                if line.startswith("[") and line.endswith("]"):
                    section = line.strip("[]").upper()
                    continue

                parts = line.split()
                if not parts:
                    continue

                if section in node_sections:
                    # first token is node id
                    self.nodes.append(parts[0])

                elif section in link_sections:
                    # id, from_node, to_node are the first 3 tokens across these sections
                    if len(parts) >= 3:
                        lid, u, v = parts[0], parts[1], parts[2]
                        self.links.append(lid)
                        self.link_from_to[lid] = (u, v)

                elif section == "COORDINATES":
                    # NodeID  X  Y
                    if len(parts) >= 3:
                        nid, x, y = parts[0], parts[1], parts[2]
                        try:
                            self.node_xy[nid] = (float(x), float(y))
                        except Exception:
                            pass

        # ensure uniqueness / stable order
        self.nodes = list(dict.fromkeys(self.nodes))
        self.links = list(dict.fromkeys(self.links))

    # --------------------------------------
    # RPT parsing (summaries + time series)
    # --------------------------------------
    def _parse_rpt(self):
        with open(self.rpt_path, "r", errors="ignore") as f:
            lines = f.readlines()

        self._parse_node_flooding_summary(lines)
        self._parse_link_flow_summary(lines)
        self._parse_node_time_series(lines)
        self._parse_link_time_series(lines)

    def _parse_node_flooding_summary(self, lines):
        """
        Your RPT has 'Node Flooding Summary' with columns:
        Node, Hours Flooded, Maximum Rate, (days hr:min), Total Flood Volume, Maximum Ponded Depth
        We'll take the last two numeric tokens as (total_volume, max_depth),
        and the first two after the ID as (hours_flooded, max_rate).
        """
        parsing = False
        skip = 0
        out = {}
        for raw in lines:
            line = raw.rstrip("\n")
            if "Node Flooding Summary" in line:
                parsing = True; skip = 2
                continue
            if parsing:
                if not line.strip():
                    parsing = False; continue
                if skip > 0: skip -= 1; continue
                if line.startswith("----") or line.startswith("Page "):
                    continue
                parts = line.split()
                if len(parts) >= 6:
                    nid = parts[0]
                    try:
                        hours_flooded = float(parts[1])
                        max_rate = float(parts[2])
                        # last two numeric tokens
                        max_depth = float(parts[-1])
                        total_volume = float(parts[-2])
                        out[nid] = {
                            "hours_flooded": hours_flooded,
                            "max_rate": max_rate,
                            "total_volume": total_volume,
                            "max_depth": max_depth,
                        }
                    except Exception:
                        # skip malformed rows
                        pass
        self.node_results = out

    def _parse_link_flow_summary(self, lines):
        """
        Your RPT 'Link Flow Summary' has:
          Link  Type  |Flow|  (days hr:min)  |Veloc|  Max/Full Flow  Max/Full Depth
        We'll parse max_flow, max_velocity, percent_full = 100 * (Max/Full Depth)
        plus type and from/to (from INP).
        """
        parsing = False
        skip = 0
        out = {}
        for raw in lines:
            line = raw.rstrip("\n")
            if "Link Flow Summary" in line:
                parsing = True; skip = 3  # skip lines under title + header underline
                continue
            if parsing:
                if not line.strip():
                    parsing = False; continue
                if skip > 0: skip -= 1; continue
                if line.startswith("----") or line.startswith("Page "):
                    continue
                parts = line.split()
                if len(parts) >= 7:
                    lid = parts[0]
                    ltype = parts[1]
                    try:
                        max_flow = float(parts[2])
                        max_velocity = float(parts[-3])
                        max_full_depth_ratio = float(parts[-1])
                        percent_full = 100.0 * max_full_depth_ratio
                    except Exception:
                        max_flow = max_velocity = percent_full = float("nan")
                    u, v = self.link_from_to.get(lid, (None, None))
                    out[lid] = {
                        "type": ltype,
                        "from_node": u, "to_node": v,
                        "max_flow": max_flow,
                        "max_velocity": max_velocity,
                        "percent_full": percent_full,
                    }
        self.link_results = out

    # ---------- time series ----------
    def _parse_node_time_series(self, lines):
        mode = "SEARCH"
        current_id = None
        cols_known = False
        order = ("inflow", "flooding", "depth", "head")
        first_dt_for_id = {}

        for raw in lines:
            line = raw.rstrip("\n")

            if "Node Results" in line:
                mode = "IN_SECTION"; current_id = None; cols_known = False; order = ("inflow", "flooding", "depth", "head"); first_dt_for_id = {}
                continue

            if mode == "IN_SECTION":
                m = _NODE_MARK.match(line)
                if m:
                    current_id = m.group(1)
                    cols_known = False
                    if current_id not in self.node_ts:
                        self.node_ts[current_id] = {"t_hours": [], "inflow": [], "flooding": [], "depth": [], "head": []}
                    continue

                if not cols_known and "Inflow" in line and "Depth" in line and "Head" in line:
                    cols_known = True
                    continue

                if not line.strip():
                    current_id = None; cols_known = False
                    continue

                if current_id and cols_known:
                    mm = _TIME_NODELINK.match(line)
                    if not mm:
                        continue
                    date_s, hh, mmn, rest = mm.groups()
                    tokens = re.split(r'\s+', rest.strip())
                    try:
                        dt = datetime.strptime(f"{date_s} {hh}:{mmn}:00", "%b-%d-%Y %H:%M:%S")
                    except Exception:
                        dt = None
                    t_hours = None
                    if dt is not None:
                        base = first_dt_for_id.get(current_id)
                        if base is None:
                            first_dt_for_id[current_id] = dt
                            base = dt
                        t_hours = (dt - base).total_seconds() / 3600.0
                    else:
                        t_hours = (int(hh) * 60 + int(mmn)) / 60.0

                    vals = []
                    for i in range(4):
                        try:
                            vals.append(float(tokens[i]))
                        except Exception:
                            vals.append(float("nan"))

                    ts = self.node_ts[current_id]
                    ts["t_hours"].append(t_hours)
                    for key, val in zip(order, vals):
                        ts[key].append(val)

            if mode == "IN_SECTION" and "Link Results" in line:
                mode = "SEARCH"; current_id = None; cols_known = False; first_dt_for_id = {}

    def _parse_link_time_series(self, lines):
        mode = "SEARCH"
        current_id = None
        cols_known = False
        order = ("flow", "velocity", "depth", "percent_full")
        first_dt_for_id = {}

        for raw in lines:
            line = raw.rstrip("\n")

            if "Link Results" in line:
                mode = "IN_SECTION"; current_id = None; cols_known = False; order = ("flow", "velocity", "depth", "percent_full"); first_dt_for_id = {}
                continue

            if mode == "IN_SECTION":
                m = _LINK_MARK.match(line)
                if m:
                    current_id = m.group(1)
                    cols_known = False
                    if current_id not in self.link_ts:
                        self.link_ts[current_id] = {"t_hours": [], "flow": [], "velocity": [], "depth": [], "percent_full": []}
                    continue

                if not cols_known and "Flow" in line and "Velocity" in line and "Depth" in line and "Percent" in line:
                    cols_known = True
                    continue

                if not line.strip():
                    current_id = None; cols_known = False
                    continue

                if current_id and cols_known:
                    mm = _TIME_NODELINK.match(line)
                    if not mm:
                        continue
                    date_s, hh, mmn, rest = mm.groups()
                    tokens = re.split(r'\s+', rest.strip())
                    try:
                        dt = datetime.strptime(f"{date_s} {hh}:{mmn}:00", "%b-%d-%Y %H:%M:%S")
                    except Exception:
                        dt = None
                    if dt is not None:
                        base = first_dt_for_id.get(current_id)
                        if base is None:
                            first_dt_for_id[current_id] = dt
                            base = dt
                        t_hours = (dt - base).total_seconds() / 3600.0
                    else:
                        t_hours = (int(hh) * 60 + int(mmn)) / 60.0

                    vals = []
                    for i in range(4):
                        try:
                            vals.append(float(tokens[i]))
                        except Exception:
                            vals.append(float("nan"))

                    ts = self.link_ts[current_id]
                    ts["t_hours"].append(t_hours)
                    for key, val in zip(order, vals):
                        ts[key].append(val)

            if mode == "IN_SECTION" and "Analysis Options" in line:
                mode = "SEARCH"; current_id = None; cols_known = False; first_dt_for_id = {}